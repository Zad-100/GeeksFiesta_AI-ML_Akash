{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df = pd.read_csv('Data/trainData.csv') # creating a dataframe\r\n",
    "                                             # for the given csv file"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DATA CLEANING"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# now we find the percentage of null/NaN values in the data\r\n",
    "rows, columns = train_df.shape # assign the rows and columns variables\r\n",
    "cellCount = rows * columns\r\n",
    "numberOfNulls = train_df.isnull().sum().sum() # summing the number of missing values\r\n",
    "                                              # under each column\r\n",
    "percentageOfMissingValues = (numberOfNulls / cellCount) * 100\r\n",
    "print(percentageOfMissingValues)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clearly, we see that a very small percentage of the entire dataset is missing or having\r\n",
    "## a NaN value. This means that we can drop the rows which contain missing values without\r\n",
    "## affecting the dataset much"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# remove all the rows containing NaN/missing values\r\n",
    "train_df = train_df.dropna()\r\n",
    "train_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.isnull().sum() # verifying if the number of missing values have been dropped"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now we need to make sure that each of the data under each column falls under the\r\n",
    "## reasonable allowed range or data-type of the corresponding column header\r\n",
    "## For example, we need to make sure that year or months or days must not have any other\r\n",
    "## values except for positive integers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.dtypes # to see the overview of the datatypes under each column right now"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## We see that 'year' has a data type of object while it should have been int64\r\n",
    "## And similarly for 'month', 'day', 'pressure' and so on...\r\n",
    "## Now if the column has anomalous values we replce it by either the mean or median\r\n",
    "## of that corresponding column.\r\n",
    "## If the column has no anomalous values but the data type set is wrong, we set it\r\n",
    "## with the right data type."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the serial number as unsigned int\r\n",
    "train_df['Unnamed: 0'] = train_df['Unnamed: 0'].astype(np.unsignedinteger)\r\n",
    "\r\n",
    "\r\n",
    "train_df['year'] = train_df['year'].astype(np.unsignedinteger) # setting it as\r\n",
    "                                                               # unsigned int\r\n",
    "replacementValue = train_df['year'].median()\r\n",
    "# first using mask() function to mark the changes for only the column being considered\r\n",
    "train_df.year = train_df.year.mask(train_df.year.lt(0), replacementValue)\r\n",
    "train_df['year'] = train_df['year'].astype(np.unsignedinteger) # re-setting to uint\r\n",
    "\r\n",
    "# for \"month\"\r\n",
    "replacementValue = train_df['month'].median()\r\n",
    "train_df.month = train_df.month.mask(train_df.month.lt(0), replacementValue)\r\n",
    "train_df['month'] = train_df['month'].astype(np.unsignedinteger)\r\n",
    "\r\n",
    "# for \"day\"\r\n",
    "replacementValue = train_df['day'].median()\r\n",
    "train_df.day = train_df.day.mask(train_df.day.lt(0), replacementValue)\r\n",
    "train_df['day'] = train_df['day'].astype(np.unsignedinteger)\r\n",
    "\r\n",
    "# for \"hour\"\r\n",
    "replacementValue = train_df['hour'].median()\r\n",
    "train_df.hour = train_df.hour.mask(train_df.hour.lt(0), replacementValue)\r\n",
    "train_df['hour'] = train_df['hour'].astype(np.unsignedinteger)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_df.dtypes)\r\n",
    "train_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df['pressure'] = train_df['pressure'].astype(np.float64)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## We see here that under the \"presure\" column, only one entry is in string data type.\r\n",
    "## So we shall replace it by the median under the column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.at[0, \"pressure\"] = 0 # converting all data to float type first\r\n",
    "train_df['pressure'] = train_df['pressure'].astype(np.float64) # setting the data type\r\n",
    "                                                               # of column to float\r\n",
    "train_df.at[0, \"pressure\"] = train_df['pressure'].median() # replaceing by median"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All the data types of the corresponding columns are correct now."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# We rename the column \"Unnamed: 0\" to \"Train_SerialNo\"\r\n",
    "train_df = train_df.rename(columns={\"Unnamed: 0\":\"Train_SerialNo\"})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DATA DESCRIPTION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Describing the data frame using describe() function\r\n",
    "print(train_df.info())\r\n",
    "train_df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Most frequent values (mode) under each column\r\n",
    "print(\"\\nThe mode values are :-\\n\")\r\n",
    "(train_df.mode())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Mean value under each column\r\n",
    "print(\"\\nThe mean values are :-\\n\")\r\n",
    "train_df.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Median values under each column\r\n",
    "print(\"\\nThe median values are :-\\n\")\r\n",
    "train_df.median()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "interpreter": {
   "hash": "75179d693a8b031e1e584314f1056063b779da9ecb5bb166829d1319e38c6634"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}